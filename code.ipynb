{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moment Prediction with LSTM using Leave-One-Subject-Out (LOSO) Cross-Validation\n",
    "\n",
    "This notebook implements a machine learning pipeline to predict knee and ankle moments from 6-axis accelerometer data. \n",
    "\n",
    "### Workflow:\n",
    "1.  **Subject Selection**: A specific subject is chosen to be the 'left-out' test set.\n",
    "2.  **Data Loading**: The model loads data from all other subjects to form the training set.\n",
    "3.  **Preprocessing**: A `MinMaxScaler` is fitted **only on the training set** to prevent data leakage. Both training and test sets are then scaled and shaped into sequences for the LSTM.\n",
    "4.  **Model Training**: A bidirectional LSTM model is trained on the training data, using a subset for validation to prevent overfitting (Early Stopping).\n",
    "5.  **Model Saving**: The best trained model is saved in both PyTorch (`.pth`) and ONNX (`.onnx`) formats.\n",
    "6.  **Evaluation**: The model is evaluated on the held-out test subject, and the predictions are saved to a CSV file for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set up environment\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- CHOOSE THE SUBJECT TO LEAVE OUT ---\n",
    "SUBJECT_TO_LEAVE_OUT = 5 # Change this value to select a different subject (e.g., 1 to 10)\n",
    "DATA_DIR = 'data' # Set the path to your data directory\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f'Leaving out subject: {SUBJECT_TO_LEAVE_OUT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading for LOSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_loso_data(subject_to_leave_out, data_dir):\n",
    "    '''Loads data based on the Leave-One-Subject-Out principle.'''\n",
    "    train_trials = []\n",
    "    test_trials = []\n",
    "    all_subjects = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d)) and d.startswith('subj')]\n",
    "\n",
    "    for subj_folder in tqdm(all_subjects, desc='Loading Subject Data'):\n",
    "        subj_id = int(subj_folder.replace('subj', ''))\n",
    "        subj_path = os.path.join(data_dir, subj_folder)\n",
    "\n",
    "        try:\n",
    "            # Inputs (Accelerations)\n",
    "            l_knee_x = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_LLML_Acc_axis1.csv'))\n",
    "            l_knee_y = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_LLML_Acc_axis2.csv'))\n",
    "            l_knee_z = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_LLML_Acc_axis3.csv'))\n",
    "            r_knee_x = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_RLML_Acc_axis1.csv'))\n",
    "            r_knee_y = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_RLML_Acc_axis2.csv'))\n",
    "            r_knee_z = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_RLML_Acc_axis3.csv'))\n",
    "            # Outputs (Moments) - Using axis2 as in the original notebook\n",
    "            l_moment = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_left_knee_moment_axis2.csv'))\n",
    "            r_moment = pd.read_csv(os.path.join(subj_path, f'subj{subj_id}_right_knee_moment_axis2.csv'))\n",
    "        except FileNotFoundError as e:\n",
    "            print(f'Skipping subject {subj_id} due to missing file: {e}')\n",
    "            continue\n",
    "\n",
    "        n_trials = l_knee_x.shape[1]\n",
    "        for trial in range(n_trials):\n",
    "            acc_data = np.stack([\n",
    "                l_knee_x.iloc[:, trial], l_knee_y.iloc[:, trial], l_knee_z.iloc[:, trial],\n",
    "                r_knee_x.iloc[:, trial], r_knee_y.iloc[:, trial], r_knee_z.iloc[:, trial]\n",
    "            ], axis=1) # Shape: (timesteps, 6)\n",
    "            moment_data = np.stack([\n",
    "                l_moment.iloc[:, trial], r_moment.iloc[:, trial]\n",
    "            ], axis=1) # Shape: (timesteps, 2)\n",
    "\n",
    "            # Allocate to train or test set\n",
    "            if subj_id == subject_to_leave_out:\n",
    "                test_trials.append((acc_data, moment_data))\n",
    "            else:\n",
    "                train_trials.append((acc_data, moment_data))\n",
    "\n",
    "    print(f'Data loaded.')\n",
    "    print(f'{len(train_trials)} trials in the training set.')\n",
    "    print(f'{len(test_trials)} trials in the test set.')\n",
    "    return train_trials, test_trials\n",
    "\n",
    "train_trial_data, test_trial_data = load_loso_data(SUBJECT_TO_LEAVE_OUT, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing for LOSO\n",
    "\n",
    "This section defines two functions:\n",
    "1. `create_sequences`: This function takes time-series data and converts it into overlapping sequences, which is the required input format for an LSTM model.\n",
    "2. `preprocess_loso_data`: This is the main preprocessing pipeline. It takes the raw training and testing trials and performs several key steps:\n",
    "    - **Scaler Fitting**: It fits a `MinMaxScaler` on the **training data only**. This is critical to prevent data leakage.\n",
    "    - **Data Scaling**: It uses the fitted scaler to transform both the training and test data.\n",
    "    - **Sequence Generation**: It calls `create_sequences` to prepare the data for the LSTM model.\n",
    "    - **Dataset Splitting**: It splits the processed training data into a final training set (85%) and a validation set (15%). The test data is kept separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, targets, seq_length, step=1):\n",
    "    \"\"\"Creates overlapping sequences and corresponding targets for a single trial.\"\"\"\n",
    "    sequences = []\n",
    "    target_seq = []\n",
    "    for i in range(0, len(features) - seq_length + 1, step):\n",
    "        sequences.append(features[i:i + seq_length])\n",
    "        target_seq.append(targets[i + seq_length - 1])  # Target is the last timestep\n",
    "    return np.array(sequences), np.array(target_seq)\n",
    "\n",
    "def preprocess_loso_data(train_trials, test_trials, seq_length=100, step=1):\n",
    "    feature_scaler = MinMaxScaler()\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    # Step 1: Fit scalers on TRAINING data only\n",
    "    print('Fitting scalers on training data...')\n",
    "    all_train_features = np.concatenate([d[0] for d in train_trials])\n",
    "    all_train_targets = np.concatenate([d[1] for d in train_trials])\n",
    "    feature_scaler.fit(all_train_features)\n",
    "    target_scaler.fit(all_train_targets)\n",
    "    del all_train_features, all_train_targets # Free memory\n",
    "\n",
    "    # Step 2: Process training data\n",
    "    all_train_feature_sequences = []\n",
    "    all_train_target_sequences = []\n",
    "    for acc_data, moment_data in tqdm(train_trials, desc=\"Processing Train Trials\"):\n",
    "        features_scaled = feature_scaler.transform(acc_data)\n",
    "        targets_scaled = target_scaler.transform(moment_data)\n",
    "        X_seq, y_seq = create_sequences(features_scaled, targets_scaled, seq_length, step)\n",
    "        all_train_feature_sequences.append(X_seq)\n",
    "        all_train_target_sequences.append(y_seq)\n",
    "\n",
    "    X_train_val = np.concatenate(all_train_feature_sequences, axis=0)  # Shape: (n_sequences, seq_length, 6)\n",
    "    y_train_val = np.concatenate(all_train_target_sequences, axis=0)  # Shape: (n_sequences, 2)\n",
    "    del all_train_feature_sequences, all_train_target_sequences # Free memory\n",
    "\n",
    "    # Step 3: Process test data\n",
    "    all_test_feature_sequences = []\n",
    "    all_test_target_sequences = []\n",
    "    for acc_data, moment_data in tqdm(test_trials, desc=\"Processing Test Trials\"):\n",
    "        features_scaled = feature_scaler.transform(acc_data)\n",
    "        targets_scaled = target_scaler.transform(moment_data)\n",
    "        X_seq, y_seq = create_sequences(features_scaled, targets_scaled, seq_length, step)\n",
    "        all_test_feature_sequences.append(X_seq)\n",
    "        all_test_target_sequences.append(y_seq)\n",
    "\n",
    "    X_test = np.concatenate(all_test_feature_sequences, axis=0)\n",
    "    y_test = np.concatenate(all_test_target_sequences, axis=0)\n",
    "    del all_test_feature_sequences, all_test_target_sequences # Free memory\n",
    "\n",
    "    # Step 4: Convert to PyTorch Tensors and create Datasets\n",
    "    train_val_dataset = TensorDataset(torch.tensor(X_train_val, dtype=torch.float32), torch.tensor(y_train_val, dtype=torch.float32))\n",
    "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "    # Step 5: Split training data into train and validation sets (85/15 split)\n",
    "    train_size = int(0.85 * len(train_val_dataset))\n",
    "    val_size = len(train_val_dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [train_size, val_size])\n",
    "\n",
    "    print(f'Total training sequences: {len(train_dataset)}')\n",
    "    print(f'Total validation sequences: {len(val_dataset)}')\n",
    "    print(f'Total test sequences: {len(test_dataset)}')\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, feature_scaler, target_scaler\n",
    "\n",
    "# Execute preprocessing\n",
    "seq_length = 100\n",
    "step = 1\n",
    "train_dataset, val_dataset, test_dataset, feature_scaler, target_scaler = preprocess_loso_data(train_trial_data, test_trial_data, seq_length, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. DataLoader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0)\n",
    "\n",
    "print(f'Created DataLoaders with batch size {batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Definition ---\n",
    "class AdvancedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_prob):\n",
    "        super(AdvancedLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout_prob if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_time_step_out = lstm_out[:, -1, : ]\n",
    "        out = self.dropout(last_time_step_out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "print('Model class defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Hyperparameters and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Hyperparameters ---\n",
    "INPUT_SIZE = 6  # 6 acceleration channels\n",
    "HIDDEN_SIZE = 64\n",
    "NUM_LAYERS = 2\n",
    "OUTPUT_SIZE = 2  # LY and RY moments\n",
    "DROPOUT_PROB = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "# Clear CUDA memory if available\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Initialize the model\n",
    "model = AdvancedLSTMModel(\n",
    "    INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE, DROPOUT_PROB\n",
    "    ).to(device)\n",
    "\n",
    "print(f'Model Architecture Initialized:{model}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training with Early Stopping ---\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)  # Added L2 regularization\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "best_model_state = None\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "num_train_batches = len(train_loader)\n",
    "num_val_batches = len(val_loader)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [T]', leave=False)\n",
    "    for seq, labels in train_pbar:\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "    avg_train_loss = total_train_loss / num_train_batches\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} [V]', leave=False)\n",
    "    for seq, labels in val_pbar:\n",
    "        seq, labels = seq.to(device), labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(seq)\n",
    "            val_loss = criterion(y_val_pred, labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "    avg_val_loss = total_val_loss / num_val_batches\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS} -> Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}')\n",
    "\n",
    "    # --- Early Stopping Check ---\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f'Early stopping triggered after {epoch+1} epochs.')\n",
    "        break\n",
    "\n",
    "# --- Save the Best Model ---\n",
    "if best_model_state is not None:\n",
    "    model_path_pth = f'model_without_subj_{SUBJECT_TO_LEAVE_OUT}.pth'\n",
    "    torch.save(best_model_state, model_path_pth)\n",
    "    print(f'Best model saved to {model_path_pth}')\n",
    "    # Load the best performing model for final evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "else:\n",
    "    print('Training did not improve, no model saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Export Model to ONNX\n",
    "\n",
    "ONNX (Open Neural Network Exchange) is a standard format for representing machine learning models. Exporting the model to ONNX allows it to be used on different platforms and frameworks (e.g., in C++, C#, or directly in game engines like Unity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Export the best model to ONNX format ---\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    model.eval()\n",
    "\n",
    "    # Create a dummy input with the correct shape for the model\n",
    "    # Shape: (batch_size, sequence_length, input_size)\n",
    "    dummy_input = torch.randn(1, seq_length, INPUT_SIZE).to(device)\n",
    "    model_path_onnx = f'model_without_subj_{SUBJECT_TO_LEAVE_OUT}.onnx'\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model,                       # model being run\n",
    "        dummy_input,                 # model input (or a tuple for multiple inputs)\n",
    "        model_path_onnx,             # where to save the model\n",
    "        export_params=True,        # store the trained parameter weights inside the model file\n",
    "        opset_version=11,          # the ONNX version to export the model to\n",
    "        do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "        input_names=['input'],     # the model's input names\n",
    "        output_names=['output'],   # the model's output names\n",
    "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}} # dynamic batch size\n",
    "    )\n",
    "    print(f'Model successfully exported to {model_path_onnx}')\n",
    "else:\n",
    "    print('No model was saved, skipping ONNX export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Final Evaluation and Saving Predictions\n",
    "\n",
    "The final step is to evaluate the performance of our best model on the unseen test set (the left-out subject). We calculate the Mean Squared Error (MSE) and the R-squared (R²) score. The predictions and the actual ground truth values are then saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Final Evaluation ---\n",
    "if best_model_state is not None:\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_ground_truth = []\n",
    "    with torch.no_grad():\n",
    "        for seq, labels in tqdm(test_loader, desc=\"Evaluating on Test Set\"):\n",
    "            seq, labels = seq.to(device), labels.to(device)\n",
    "            predictions = model(seq)\n",
    "            all_predictions.append(predictions.cpu().numpy())\n",
    "            all_ground_truth.append(labels.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_ground_truth = np.concatenate(all_ground_truth, axis=0)\n",
    "\n",
    "    # Rescale predictions and ground truth back to original units\n",
    "    predictions_rescaled = target_scaler.inverse_transform(all_predictions)\n",
    "    ground_truth_rescaled = target_scaler.inverse_transform(all_ground_truth)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    mse = mean_squared_error(ground_truth_rescaled, predictions_rescaled)\n",
    "    r2 = r2_score(ground_truth_rescaled, predictions_rescaled)\n",
    "    print(f'--- Test Set Performance (Subject {SUBJECT_TO_LEAVE_OUT}) ---')\n",
    "    print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
    "    print(f'R-squared (R²): {r2:.4f}')\n",
    "\n",
    "    # --- Save predictions to CSV ---\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual_LY_Moment': ground_truth_rescaled[:, 0],\n",
    "        'Predicted_LY_Moment': predictions_rescaled[:, 0],\n",
    "        'Actual_RY_Moment': ground_truth_rescaled[:, 1],\n",
    "        'Predicted_RY_Moment': predictions_rescaled[:, 1]\n",
    "    })\n",
    "    results_filename = f'predictions_for_subj_{SUBJECT_TO_LEAVE_OUT}.csv'\n",
    "    results_df.to_csv(results_filename, index=False)\n",
    "    print(f'Predictions saved to {results_filename}')\n",
    "\n",
    "    # --- Plot training history ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history['train_loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    # --- Visualize predictions vs. ground truth on a sample from the test set ---\n",
    "    plot_len = min(1000, len(ground_truth_rescaled)) # Plot up to 1000 points\n",
    "    time_steps = np.arange(plot_len)\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    # Plot LY Moment\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(time_steps, ground_truth_rescaled[:plot_len, 0], label='Actual LY Moment', color='blue', alpha=0.8)\n",
    "    plt.plot(time_steps, predictions_rescaled[:plot_len, 0], label='Predicted LY Moment', color='red', linestyle='--')\n",
    "    plt.title(f'Test Set Performance: LY Moment Prediction (Subject {SUBJECT_TO_LEAVE_OUT})')\n",
    "    plt.ylabel('Moment')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot RY Moment\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(time_steps, ground_truth_rescaled[:plot_len, 1], label='Actual RY Moment', color='green', alpha=0.8)\n",
    "    plt.plot(time_steps, predictions_rescaled[:plot_len, 1], label='Predicted RY Moment', color='orange', linestyle='--')\n",
    "    plt.title(f'Test Set Performance: RY Moment Prediction (Subject {SUBJECT_TO_LEAVE_OUT})')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Moment')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'test_set_predictions_subj_{SUBJECT_TO_LEAVE_OUT}.png')\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No model was trained, skipping evaluation.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
